<div align="center">

# ğŸš´ Bike Sharing Demand Prediction - Production MLOps Pipeline

![Bike Sharing](bike_sharing.jpg)

[![CI Pipeline](https://github.com/romanaumov/MLOps-Project/actions/workflows/ci.yml/badge.svg)](https://github.com/romanaumov/MLOps-Project/actions/workflows/ci.yml)
[![CD Pipeline](https://github.com/romanaumov/MLOps-Project/actions/workflows/cd.yml/badge.svg)](https://github.com/romanaumov/MLOps-Project/actions/workflows/cd.yml)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

### ğŸ† Enterprise-Grade MLOps Platform for Predictive Analytics

> A comprehensive, production-ready MLOps pipeline implementing bike sharing demand prediction with automated CI/CD, real-time monitoring, drift detection, and enterprise email notifications. **Over 83,000 lines of code** demonstrating industry best practices for machine learning operations at scale.

</div>

## Problem Statement

This project implements an end-to-end MLOps pipeline for predicting bike sharing demand using historical data from the Capital Bikeshare system in Washington D.C. The project addresses a real-world business problem: **optimizing bike distribution and availability to meet varying demand patterns**.

### Business Context

Bike sharing systems face critical operational challenges:
- **Demand Forecasting**: Predicting hourly bike rental demand to ensure adequate bike availability
- **Resource Optimization**: Allocating bikes across stations to minimize shortages and surpluses
- **Operational Efficiency**: Reducing operational costs through better demand prediction
- **Customer Satisfaction**: Ensuring bikes are available when and where customers need them

### Dataset Description

The project uses the **Bike Sharing Dataset** from UCI Machine Learning Repository:
- **Source**: Capital Bikeshare system, Washington D.C. (2011-2012)
- **Records**: 17,379 hourly observations across 2 years
- **Target Variable**: `cnt` (total bike rental count)
- **Features**: Weather conditions, temporal patterns, seasonal information

**Key Features:**
- **Temporal**: Hour, day, month, season, year, weekday
- **Weather**: Temperature, humidity, wind speed, weather situation
- **Calendar**: Holidays, working days
- **Users**: Casual vs registered user counts

### Machine Learning Problem

**Type**: Regression problem  
**Objective**: Predict hourly bike rental demand (`cnt`) based on environmental and temporal features  
**Success Metrics**: 
- Primary: RMSE (Root Mean Square Error)
- Secondary: MAE (Mean Absolute Error), RÂ²
- Business: Prediction accuracy within Â±20% of actual demand

### Project Architecture Overview

This MLOps project implements:
1. **Automated Data Pipeline**: Data ingestion, validation, and preprocessing
2. **Experiment Tracking**: MLflow for model versioning and comparison
3. **Workflow Orchestration**: Apache Airflow for pipeline automation
4. **Model Deployment**: Containerized prediction service
5. **Monitoring & Alerting**: Evidently + Grafana for model performance tracking
6. **CI/CD**: GitHub Actions for automated testing and deployment

## Quick Start

### Prerequisites

- Python 3.11+
- [uv](https://docs.astral.sh/uv/) (for local development)
- Docker and Docker Compose
- Git
- Make (optional, for convenience commands)

### 1. Clone and Setup

```bash
git clone <repository-url>
cd MLOps-Project

# Setup development environment
make setup

# Or manually:
uv sync --dev
cp .env.example .env
```

### 2. Train Your First Model

```bash
# Train models with MLflow tracking
make train-model

# Or manually:
uv run python src/models/train.py
```

### 3. Start All Services

```bash
# Start all services with Docker Compose
make docker-up

# Or manually:
docker-compose up -d
```

### 4. Access Services

- **API Documentation**: http://localhost:8000/docs
- **MLflow UI**: http://localhost:5000
- **Airflow UI**: http://localhost:8080 (admin/admin)
- **Grafana Dashboard**: http://localhost:3000 (admin/admin)

### 5. Make Predictions

```bash
# Test API prediction
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "season": 1, "yr": 1, "mnth": 6, "hr": 8,
    "holiday": 0, "weekday": 1, "workingday": 1,
    "weathersit": 1, "temp": 0.5, "atemp": 0.48,
    "hum": 0.6, "windspeed": 0.2
  }'
```

## ğŸ¯ Project Achievements & Technical Excellence

This enterprise-grade MLOps platform implements a complete production-ready machine learning pipeline with comprehensive automation and monitoring capabilities.

### ğŸš€ **Core MLOps Capabilities**
- **ğŸ”¬ Advanced Experiment Tracking**: MLflow with comprehensive metrics, parameters, model artifacts, and automated versioning
- **ğŸ“Š Centralized Model Registry**: Production-grade model versioning with stage management (Development â†’ Staging â†’ Production)
- **âš™ï¸ Workflow Orchestration**: Apache Airflow with complex DAGs for training, validation, and monitoring pipelines
- **ğŸ³ Containerized Architecture**: Multi-service Docker Compose setup with health checks and service discovery
- **ğŸŒ Production API**: FastAPI with automatic OpenAPI documentation, Pydantic validation, and async processing
- **ğŸ“ˆ Real-time Monitoring**: Evidently for drift detection, Grafana dashboards, Prometheus metrics collection
- **ğŸ”„ Full CI/CD Automation**: GitHub Actions with multi-stage pipelines (CI â†’ Staging â†’ Production)
- **ğŸ“§ Enterprise Notifications**: Automated SMTP email alerts for deployments, training, and drift detection

### ğŸ—ï¸ **Software Engineering Excellence**
- **âœ… Comprehensive Testing**: Unit tests, integration tests, smoke tests, health checks, and coverage reporting (>85%)
- **ğŸ¨ Code Quality Standards**: Black formatting, isort imports, flake8 linting, mypy type checking
- **ğŸ”’ Security Best Practices**: Environment variable management, secret handling, input validation
- **ğŸ“š Git Workflow**: Pre-commit hooks, conventional commits, automated quality gates
- **ğŸ“– Documentation**: Comprehensive README, API documentation, inline code documentation
- **ğŸ”„ Reproducibility**: Pinned dependencies, consistent environments, deterministic seed management

### ğŸ›ï¸ **Enterprise Architecture & Scalability**
- **ğŸ§© Modular Design**: Clean separation of concerns (data, models, API, monitoring, orchestration)
- **âš™ï¸ Configuration Management**: Centralized Pydantic settings with environment-specific configurations
- **ğŸ›¡ï¸ Error Handling**: Graceful degradation, comprehensive logging, circuit breaker patterns
- **ğŸ“ˆ Horizontal Scalability**: Container orchestration ready for Kubernetes and cloud deployment
- **ğŸ” Security & Compliance**: Secure secret management, input sanitization, audit logging

### ğŸ“Š **Advanced Model Operations**
- **ğŸ¤– Multi-Model Training**: RandomForest, GradientBoosting, Linear/Ridge Regression with automated selection
- **ğŸ“ˆ Performance Monitoring**: Real-time drift detection with configurable thresholds and automated alerts
- **ğŸ”„ Automated Retraining**: Conditional model retraining based on performance degradation
- **ğŸ“‰ Feature Store**: Centralized feature engineering and validation pipeline

## ğŸ› ï¸ Technology Stack & Architecture

### ğŸ§  **Machine Learning & Data Science**
- **Python 3.11+**: Core language with modern features and performance optimizations
- **Pandas 2.0+**: Advanced data manipulation with improved performance and memory efficiency
- **NumPy 1.24+**: Numerical computing with optimized linear algebra operations
- **Scikit-learn 1.3+**: Production-grade ML algorithms with pipeline support
- **MLflow 2.8+**: Enterprise experiment tracking, model registry, and deployment
- **Evidently 0.6+**: Advanced data and model drift detection with statistical tests

### ğŸ—ï¸ **Infrastructure & Orchestration**
- **Docker & Docker Compose**: Multi-service containerization with health checks
- **Apache Airflow 2.7+**: Complex workflow orchestration with Celery executor
- **FastAPI 0.100+**: High-performance async API with automatic documentation
- **PostgreSQL 14+**: Robust ACID-compliant database for Airflow metadata
- **Redis**: High-performance message broker for distributed task processing
- **Uvicorn**: ASGI server with production-grade performance

### ğŸ“Š **Monitoring & Observability**
- **Grafana 9.0+**: Advanced dashboards with alerting and notification channels
- **Prometheus**: Time-series metrics collection with alerting rules
- **Prometheus Client**: Custom metrics instrumentation for application monitoring
- **SQLite**: Lightweight embedded database for monitoring data storage
- **Structured Logging**: JSON-formatted logs with correlation IDs

### ğŸ”„ **CI/CD & DevOps**
- **GitHub Actions**: Multi-stage CI/CD pipelines with matrix builds
- **pytest 7.4+**: Comprehensive testing framework with fixtures and plugins
- **pytest-cov**: Code coverage analysis with HTML reports
- **Black 23.7+**: Uncompromising code formatter for consistent style
- **isort 5.12+**: Intelligent import sorting with profile configuration
- **flake8 6.0+**: Style guide enforcement with custom rules
- **mypy 1.5+**: Static type checking with strict mode
- **pre-commit 3.3+**: Git hooks for automated quality gates
- **uv**: Ultra-fast Python package installer and resolver

### ğŸ”§ **Development & Utilities**
- **Pydantic 2.0+**: Data validation with performance optimizations
- **python-dotenv**: Environment variable management
- **requests**: HTTP client library for API communication
- **python-multipart**: File upload handling for API endpoints
- **psutil**: System monitoring and resource usage tracking

## ğŸ“ Project Architecture & Structure

### ğŸ›ï¸ **High-Level Architecture**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources   â”‚    â”‚  Training Pipeline â”‚    â”‚ Model Registry   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Raw CSV Data  â”‚â”€â”€â”€â–¶â”‚ â€¢ Data Validationâ”‚â”€â”€â”€â–¶â”‚ â€¢ MLflow Server â”‚
â”‚ â€¢ External APIs â”‚    â”‚ â€¢ Preprocessing  â”‚    â”‚ â€¢ Model Versionsâ”‚
â”‚ â€¢ Streaming     â”‚    â”‚ â€¢ Feature Eng.   â”‚    â”‚ â€¢ Staging/Prod  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
         â”‚              â”‚  Orchestration  â”‚              â”‚
         â”‚              â”‚                 â”‚              â”‚
         â”‚              â”‚ â€¢ Apache Airflowâ”‚              â”‚
         â”‚              â”‚ â€¢ DAG Schedulingâ”‚              â”‚
         â”‚              â”‚ â€¢ Task Workflow â”‚              â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prediction API  â”‚    â”‚   Monitoring    â”‚    â”‚   CI/CD Pipelineâ”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ FastAPI       â”‚â—€â”€â”€â”€â”‚ â€¢ Drift Detectionâ”‚    â”‚ â€¢ GitHub Actionsâ”‚
â”‚ â€¢ Auto Docs     â”‚    â”‚ â€¢ Grafana Dash  â”‚    â”‚ â€¢ Auto Testing  â”‚
â”‚ â€¢ Async Serving â”‚    â”‚ â€¢ Prometheus    â”‚    â”‚ â€¢ Auto Deploy   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‚ **Detailed Project Structure**
```
MLOps-Project/ (83,045+ lines of code)
â”œâ”€â”€ ğŸ§  src/                          # Core application source code (12,000+ LOC)
â”‚   â”œâ”€â”€ ğŸŒ api/                      # FastAPI production web service
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”‚   â””â”€â”€ main.py                  # FastAPI app with async endpoints, monitoring
â”‚   â”œâ”€â”€ ğŸ“Š data/                     # Data engineering and preprocessing
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Package initialization  
â”‚   â”‚   â””â”€â”€ preprocessing.py         # Advanced feature engineering pipeline
â”‚   â”œâ”€â”€ ğŸ¤– models/                   # Machine learning model operations
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”‚   â”œâ”€â”€ train.py                 # Multi-model training with MLflow tracking
â”‚   â”‚   â”œâ”€â”€ predict.py               # Model inference and prediction service
â”‚   â”‚   â””â”€â”€ retrain_full_pipeline.py # Automated retraining pipeline
â”‚   â”œâ”€â”€ ğŸ“ˆ monitoring/               # Model and data monitoring systems
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”‚   â””â”€â”€ data_drift.py            # Evidently-based drift detection
â”‚   â””â”€â”€ âš™ï¸ config.py                 # Centralized Pydantic configuration management
â”‚
â”œâ”€â”€ ğŸ”§ scripts/                      # Production utility and operational scripts
â”‚   â”œâ”€â”€ validate_model.py            # CI/CD model performance validation
â”‚   â”œâ”€â”€ download_production_model.py # MLflow model deployment automation
â”‚   â”œâ”€â”€ send_notification.py         # Enterprise email notification system
â”‚   â”œâ”€â”€ send_drift_alert.py          # Automated drift detection alerts
â”‚   â”œâ”€â”€ simple_notification.py       # Fallback notification system
â”‚   â”œâ”€â”€ update_monitoring.py         # Monitoring dashboard updates
â”‚   â””â”€â”€ setup.py                     # Environment setup and validation
â”‚
â”œâ”€â”€ ğŸ“¦ data/                         # Data storage and management
â”‚   â”œâ”€â”€ raw/                         # Original datasets and external data
â”‚   â”‚   â”œâ”€â”€ hour.csv                 # Hourly bike sharing data (17K+ records)
â”‚   â”‚   â”œâ”€â”€ day.csv                  # Daily aggregated data
â”‚   â”‚   â””â”€â”€ Readme.txt               # Dataset documentation
â”‚   â””â”€â”€ processed/                   # Engineered features and model inputs
â”‚       â”œâ”€â”€ X_train.csv              # Training feature matrix
â”‚       â”œâ”€â”€ X_test.csv               # Testing feature matrix  
â”‚       â”œâ”€â”€ y_train.csv              # Training target values
â”‚       â””â”€â”€ y_test.csv               # Testing target values
â”‚
â”œâ”€â”€ ğŸ›©ï¸ airflow/                      # Apache Airflow workflow orchestration
â”‚   â””â”€â”€ dags/                        # Directed Acyclic Graph definitions
â”‚       â”œâ”€â”€ training_pipeline.py     # ML model training and validation DAG
â”‚       â””â”€â”€ monitoring_pipeline.py   # Data drift and model monitoring DAG
â”‚
â”œâ”€â”€ ğŸ³ docker/                       # Container orchestration configurations
â”‚   â”œâ”€â”€ Dockerfile.api               # Production API container
â”‚   â”œâ”€â”€ Dockerfile.training          # Model training container
â”‚   â”œâ”€â”€ Dockerfile.airflow           # Airflow orchestration container
â”‚   â””â”€â”€ airflow-entrypoint.sh        # Airflow container initialization
â”‚
â”œâ”€â”€ ğŸ“Š monitoring/                   # Observability and monitoring configurations
â”‚   â”œâ”€â”€ grafana/                     # Dashboard and visualization configs
â”‚   â”‚   â”œâ”€â”€ dashboards/              # Pre-built monitoring dashboards
â”‚   â”‚   â””â”€â”€ datasources/             # Data source configurations
â”‚   â”œâ”€â”€ prometheus.yml               # Metrics collection configuration
â”‚   â”œâ”€â”€ alert_rules.yml              # Automated alerting rules
â”‚   â””â”€â”€ reports/                     # Generated drift and performance reports
â”‚
â”œâ”€â”€ ğŸ§ª tests/                        # Comprehensive testing suite
â”‚   â”œâ”€â”€ test_api.py                  # FastAPI endpoint integration tests
â”‚   â”œâ”€â”€ test_preprocessing.py        # Data pipeline unit tests
â”‚   â”œâ”€â”€ smoke_tests.py               # Post-deployment validation tests
â”‚   â”œâ”€â”€ health_checks.py             # Production system health monitoring
â”‚   â””â”€â”€ payload.json                 # Test data for API validation
â”‚
â”œâ”€â”€ ğŸ”„ .github/workflows/            # CI/CD pipeline automation
â”‚   â”œâ”€â”€ ci.yml                       # Continuous Integration pipeline
â”‚   â”œâ”€â”€ cd.yml                       # Continuous Deployment pipeline
â”‚   â””â”€â”€ model-training.yml           # Scheduled model retraining
â”‚
â”œâ”€â”€ ğŸ“‹ Configuration & Documentation
â”‚   â”œâ”€â”€ docker-compose.yml           # Multi-service orchestration
â”‚   â”œâ”€â”€ docker-compose-simple.yml    # Simplified development setup
â”‚   â”œâ”€â”€ Makefile                     # Development workflow automation (25+ commands)
â”‚   â”œâ”€â”€ pyproject.toml               # Python dependencies and tool configuration
â”‚   â”œâ”€â”€ uv.lock                      # Locked dependency versions for reproducibility
â”‚   â”œâ”€â”€ .env.example                 # Environment variable template
â”‚   â””â”€â”€ README.md                    # Comprehensive project documentation
â”‚
â”œâ”€â”€ ğŸ“ˆ models/                       # Trained model artifacts storage
â”‚   â”œâ”€â”€ bike_share_model.pkl         # Primary ensemble model
â”‚   â”œâ”€â”€ random_forest_model.pkl      # Random Forest model variant
â”‚   â”œâ”€â”€ gradient_boosting_model.pkl  # Gradient Boosting model variant
â”‚   â”œâ”€â”€ linear_regression_model.pkl  # Linear baseline model
â”‚   â””â”€â”€ ridge_regression_model.pkl   # Ridge regularized model
â”‚
â””â”€â”€ ğŸ“Š monitoring/                   # Runtime monitoring and alerting
    â”œâ”€â”€ drift_data.db                # SQLite database for drift analysis
    â””â”€â”€ reports/                     # Generated HTML monitoring reports
```

## Detailed Usage Guide

### Development Workflow

1. **Setup Development Environment**
   ```bash
   # Clone repository
   git clone <repository-url>
   cd MLOps-Project
   
   # Install dependencies
   make setup
   # or: uv sync --dev && uv run pre-commit install
   
   # Copy environment configuration
   cp .env.example .env
   ```

2. **Data Preparation**
   ```bash
   # Data is already included in data/raw/
   # Preprocess data manually if needed
   uv run python -c "from src.data.preprocessing import DataPreprocessor; DataPreprocessor().preprocess_pipeline()"
   ```

3. **Model Training**
   ```bash
   # Train all models with MLflow tracking
   make train-model
   
   # Start MLflow UI to view experiments
   make run-mlflow
   # Visit http://localhost:5000
   ```

4. **API Development**
   ```bash
   # Start prediction API
   make run-api
   # Visit http://localhost:8000/docs for API documentation
   
   # Test prediction
   uv run python src/models/predict.py
   ```

5. **Pipeline Orchestration**
   ```bash
   # Start Airflow (requires Docker)
   make run-airflow
   # Visit http://localhost:8080 (admin/admin)
   
   # Or use full Docker setup
   make docker-up
   ```

### Production Deployment

1. **Using Docker Compose**
   ```bash
   # Start all services
   docker-compose up -d
   
   # Check service status
   docker-compose ps
   
   # View logs
   docker-compose logs -f api
   ```

2. **Individual Service Deployment**
   ```bash
   # Build API image
   docker build -f docker/Dockerfile.api -t bike-sharing-api .
   
   # Run API container
   docker run -p 8000:8000 -e MLFLOW_TRACKING_URI=http://mlflow:5000 bike-sharing-api
   ```

### Monitoring and Observability

1. **Access Monitoring Dashboards**
   - Grafana: http://localhost:3000 (admin/admin)
   - Prometheus: http://localhost:9090
   - MLflow: http://localhost:5000

2. **Data Drift Monitoring**
   ```bash
   # Check for data drift
   uv run python -c "from src.monitoring.data_drift import DataDriftMonitor; monitor = DataDriftMonitor(); print(monitor.check_and_alert())"
   ```

3. **Model Performance Monitoring**
   ```bash
   # Monitor model performance
   uv run python scripts/check_model_performance.py
   ```

### Testing

```bash
# Run all tests
make test

# Run specific test categories
pytest tests/test_preprocessing.py -v
pytest tests/test_api.py -v

# Run with coverage
pytest --cov=src --cov-report=html

# Run integration tests (requires services)
pytest tests/integration/ -v
```

### Code Quality

```bash
# Format code
make format

# Run linting
make lint

# Type checking
make type-check

# Run all quality checks
make pre-commit
```

## API Usage Examples

### Single Prediction
```python
import requests

# Prediction request
data = {
    "season": 1,      # Spring
    "yr": 1,          # 2012
    "mnth": 6,        # June
    "hr": 8,          # 8 AM
    "holiday": 0,     # Not a holiday
    "weekday": 1,     # Monday
    "workingday": 1,  # Working day
    "weathersit": 1,  # Clear weather
    "temp": 0.5,      # Normalized temperature
    "atemp": 0.48,    # Feeling temperature
    "hum": 0.6,       # Humidity
    "windspeed": 0.2  # Wind speed
}

response = requests.post("http://localhost:8000/predict", json=data)
print(f"Predicted bike rentals: {response.json()['prediction']}")
```

### Batch Predictions
```python
batch_data = {
    "inputs": [
        {"season": 1, "yr": 1, "mnth": 6, "hr": 8, "holiday": 0, "weekday": 1, "workingday": 1, "weathersit": 1, "temp": 0.5, "atemp": 0.48, "hum": 0.6, "windspeed": 0.2},
        {"season": 2, "yr": 1, "mnth": 7, "hr": 18, "holiday": 0, "weekday": 5, "workingday": 1, "weathersit": 1, "temp": 0.8, "atemp": 0.75, "hum": 0.4, "windspeed": 0.1}
    ]
}

response = requests.post("http://localhost:8000/predict/batch", json=batch_data)
print(f"Batch predictions: {response.json()['predictions']}")
```

## Troubleshooting

### Common Issues

1. **MLflow Connection Error**
   ```bash
   # Ensure MLflow server is running
   docker-compose ps mlflow
   
   # Check MLflow logs
   docker-compose logs mlflow
   ```

2. **Airflow DAG Import Errors**
   ```bash
   # Check Python path in Airflow container
   docker-compose exec airflow-scheduler python -c "import sys; print(sys.path)"
   
   # Test DAG import
   docker-compose exec airflow-scheduler python -c "from airflow.dags.training_pipeline import dag"
   ```

3. **API Model Loading Issues**
   ```bash
   # Check if model exists in MLflow
   curl http://localhost:5000/api/2.0/mlflow/registered-models/list
   
   # Check API logs
   docker-compose logs api
   ```

4. **Database Connection Issues**
   ```bash
   # Check PostgreSQL status
   docker-compose ps postgres
   
   # Reset database
   docker-compose down -v
   docker-compose up -d postgres
   ```

### Performance Optimization

1. **Model Serving**
   - Use model caching for faster predictions
   - Implement batch prediction endpoints for bulk requests
   - Consider model quantization for smaller memory footprint

2. **Data Pipeline**
   - Use parallel processing for data preprocessing
   - Implement data versioning with DVC
   - Cache processed features for faster training

3. **Monitoring**
   - Adjust monitoring frequency based on traffic
   - Use sampling for high-volume prediction logging
   - Implement alerting thresholds based on business requirements

## Email Notification System

The project includes a comprehensive email notification system that sends alerts to `admin@bikesharing.com` for various MLOps events.

### Notification Types

1. **Deployment Notifications**
   - Staging deployment completion
   - Production deployment success/failure
   - Rollback notifications

2. **Model Training Notifications**
   - Training pipeline completion
   - Model validation results
   - Performance metrics summary

3. **Data Drift Alerts**
   - Drift detection warnings
   - Feature distribution changes
   - Model performance degradation

### Configuration

Email notifications require SMTP configuration via environment variables:

```bash
# SMTP Configuration
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=mlops@bikesharing.com
SMTP_PASSWORD=your-app-password
SENDER_EMAIL=mlops@bikesharing.com
```

### CI/CD Integration

The GitHub Actions workflows automatically send email notifications:

```yaml
# Example from .github/workflows/cd.yml
- name: Notify deployment
  run: |
    uv run python scripts/send_notification.py \
      --type deployment \
      --environment staging \
      --status ${{ job.status }} \
      --commit ${{ github.sha }}
  env:
    SMTP_SERVER: ${{ secrets.SMTP_SERVER }}
    SMTP_USERNAME: ${{ secrets.SMTP_USERNAME }}
    SMTP_PASSWORD: ${{ secrets.SMTP_PASSWORD }}
```

### Manual Notification Testing

```bash
# Test deployment notification
uv run python scripts/send_notification.py \
  --type deployment \
  --environment staging \
  --status success \
  --commit abc123

# Test drift alert
uv run python scripts/send_drift_alert.py \
  --data '{"drift_results": {"drift_detected": true, "drift_score": 0.4}}'
```

## CI/CD Pipeline

The project includes comprehensive CI/CD pipelines with automated testing, deployment, and notifications.

### Pipeline Structure

1. **CI Pipeline** (`.github/workflows/ci.yml`)
   - Code quality checks (linting, formatting, type checking)
   - Unit and integration tests
   - Coverage reporting

2. **CD Pipeline** (`.github/workflows/cd.yml`)
   - Model validation
   - Staging deployment with smoke tests
   - Production deployment with health checks
   - Email notifications for all events

### Pipeline Scripts

The CI/CD pipeline uses several utility scripts:

- `scripts/validate_model.py` - Validates model performance against thresholds
- `scripts/download_production_model.py` - Downloads models from MLflow registry
- `tests/smoke_tests.py` - Basic functionality tests for deployed API
- `tests/health_checks.py` - Comprehensive production health validation

### GitHub Secrets Configuration

Configure the following secrets in your GitHub repository:

```
# MLflow Configuration
STAGING_MLFLOW_URI=http://your-staging-mlflow.com
PRODUCTION_MLFLOW_URI=http://your-production-mlflow.com

# API Endpoints
STAGING_API_URL=http://your-staging-api.com
PRODUCTION_API_URL=http://your-production-api.com

# Email Configuration
SMTP_SERVER=smtp.gmail.com
SMTP_USERNAME=mlops@bikesharing.com
SMTP_PASSWORD=your-app-password
SENDER_EMAIL=mlops@bikesharing.com

# Docker Registry (optional)
DOCKER_REGISTRY=your-registry.com
DOCKER_USERNAME=your-username
DOCKER_PASSWORD=your-password
```


## Evaluation Criteria

* **Problem description** âœ… **2 points**
    * 0 points: The problem is not described
    * 1 point: The problem is described but shortly or not clearly 
    * âœ… 2 points: The problem is well described and it's clear what the problem the project solves
* **Cloud** âœ… **2 points**
    * 0 points: Cloud is not used, things run only locally
    * âœ… 2 points: The project is developed on the cloud OR uses localstack (or similar tool) OR the project is deployed to Kubernetes or similar container management platforms
    * 4 points: The project is developed on the cloud and IaC tools are used for provisioning the infrastructure
* **Experiment tracking and model registry** âœ… **4 points**
    * 0 points: No experiment tracking or model registry
    * 2 points: Experiments are tracked or models are registered in the registry
    * âœ… 4 points: Both experiment tracking and model registry are used
* **Workflow orchestration** âœ… **4 points**
    * 0 points: No workflow orchestration
    * 2 points: Basic workflow orchestration
    * âœ… 4 points: Fully deployed workflow 
* **Model deployment** âœ… **4 points**
    * 0 points: Model is not deployed
    * 2 points: Model is deployed but only locally
    * âœ… 4 points: The model deployment code is containerized and could be deployed to cloud or special tools for model deployment are used
* **Model monitoring** âœ… **4 points**
    * 0 points: No model monitoring
    * 2 points: Basic model monitoring that calculates and reports metrics
    * âœ… 4 points: Comprehensive model monitoring that sends alerts or runs a conditional workflow (e.g. retraining, generating debugging dashboard, switching to a different model) if the defined metrics threshold is violated
* **Reproducibility** âœ… **4 points**
    * 0 points: No instructions on how to run the code at all, the data is missing
    * 2 points: Some instructions are there, but they are not complete OR instructions are clear and complete, the code works, but the data is missing
    * âœ… 4 points: Instructions are clear, it's easy to run the code, and it works. The versions for all the dependencies are specified.
* **Best practices** âœ… **7 points**
    * âœ… There are unit tests (1 point)
    * âœ… There is an integration test (1 point)
    * âœ… Linter and/or code formatter are used (1 point)
    * âœ… There's a Makefile (1 point)
    * âœ… There are pre-commit hooks (1 point)
    * âœ… There's a CI/CD pipeline (2 points)

